{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doc2Vec Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_reviews.to_csv('../data/all_reviews.csv')\n",
    "\n",
    "all_reviews.title = all_reviews.title.map(lambda x: re.sub('[^a-zA-Z0-9 \\n\\.]', '', x.lower()))\n",
    "all_reviews.pros = all_reviews.pros.map(lambda x: re.sub('[^a-zA-Z0-9 \\n\\.]', '', x.lower()))\n",
    "all_reviews.cons = all_reviews.cons.map(lambda x: re.sub('[^a-zA-Z0-9 \\n\\.]', '', x.lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import TaggedDocument, Doc2Vec\n",
    "\n",
    "stemmed_cons_list = stemmed_cons.apply(lambda x: join_word_list(x))\n",
    "X = all_reviews['pros'].values.tolist()\n",
    "X[:3]\n",
    "\n",
    "documents = []\n",
    "# input is a list of strings\n",
    "for i, doc in enumerate(X):\n",
    "    documents.append(TaggedDocument(words = doc, tags = [i]))\n",
    "documents[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Doc2Vec(vector_size=200, dbow_words= 1, dm=0, epochs=1,  window=5, seed=1337, min_count=30, workers=4,alpha=0.025, min_alpha=0.025)\n",
    "model.build_vocab(documents)\n",
    "for epoch in range(10):\n",
    "    print(\"epoch \"+str(epoch))\n",
    "    model.train(documents, total_examples=1000, epochs=1)\n",
    "    model.save('cyber-trend-index-dataset.model')\n",
    "    model.alpha -= 0.002  # decrease the learning rate\n",
    "    model.min_alpha = model.alpha  # fix the learning rate, no decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(str):\n",
    "    # remove hyper links\n",
    "    str = re.sub(r'http(s)?:\\/\\/\\S*? ', \"\", str)\n",
    "    return str\n",
    "\n",
    "# takes string, removes special characters, returns tokenized list\n",
    "def preprocess_document(text):\n",
    "    text = preprocess(text)\n",
    "    return ''.join([x if x.isalnum() or x.isspace() else \" \" for x in text ]).split()\n",
    "\n",
    "preprocess_document('hello a asoi athis 920 8 -')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.cluster import KMeansClusterer\n",
    "from gensim import models\n",
    "\n",
    "dataset = [tokenize_sentences(sentence) for sentence in X]\n",
    "dataset[:3]\n",
    "\n",
    "# import gensim.downloader as api\n",
    "# from gensim.models import TfidfModel\n",
    "# from gensim.corpora import Dictionary\n",
    "\n",
    "# dct = Dictionary(dataset)  # fit dictionary\n",
    "# corpus = [dct.doc2bow(line) for line in dataset]  # convert corpus to BoW format\n",
    "\n",
    "# tfidf_model = TfidfModel(corpus)  # fit model\n",
    "# vectors = tfidf_model[corpus]  # apply model to the first corpus document\n",
    "\n",
    "# import numpy as np\n",
    "# np.shape(vectors[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = []\n",
    "for doc in X:\n",
    "    vectors.append(model.infer_vector(preprocess_document(doc)))\n",
    "\n",
    "# kclusterer = KMeansClusterer(num_means =10, distance=nltk.cluster.util.cosine_distance, repeats=25)\n",
    "kclusterer = KMeansClusterer(num_means =15, distance=nltk.cluster.util.cosine_distance)\n",
    "assigned_clusters = kclusterer.cluster(vectors, assign_clusters=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def get_titles_by_cluster(id):\n",
    "    list = []\n",
    "    for x in range(0, len(assigned_clusters)):\n",
    "        if (assigned_clusters[x] == id):\n",
    "            list.append(X[x])\n",
    "    return list\n",
    "\n",
    "def get_topics(titles):\n",
    "    words = [preprocess_document(x) for x in titles]\n",
    "    words = [word for sublist in words for word in sublist]\n",
    "    filtered_words = [word for word in words if word not in (stopwords.words('english')+['work','company'])]\n",
    "    count = Counter(filtered_words)\n",
    "    print(count.most_common()[:12])\n",
    "\n",
    "\n",
    "def cluster_to_topics(id):\n",
    "    get_topics(get_titles_by_cluster(id))\n",
    "\n",
    "for i in range(0,15):\n",
    "    print(cluster_to_topics(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering using TF-IDF vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "print(all_reviews.info())\n",
    "data = all_reviews\n",
    "\n",
    "tfidf = TfidfVectorizer(\n",
    "    min_df = 5,\n",
    "    max_df = 0.95,\n",
    "    max_features = 8000,\n",
    "#     stop_words = (['work', 'company', 'great','good', 'benefits'] + list(stopwords.words('english')))\n",
    "    stop_words = (['work', 'company'] + list(stopwords.words('english')))\n",
    "#     stop_words = 'english'\n",
    ")\n",
    "tfidf.fit(data.cons)\n",
    "text = tfidf.transform(data.cons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_optimal_clusters(data, max_k):\n",
    "    iters = range(2, max_k+1, 2)\n",
    "    \n",
    "    sse = []\n",
    "    for k in iters:\n",
    "        sse.append(MiniBatchKMeans(n_clusters=k, init_size=1024, batch_size=2048, random_state=20).fit(data).inertia_)\n",
    "        print('Fit {} clusters'.format(k))\n",
    "        \n",
    "    f, ax = plt.subplots(1, 1)\n",
    "    ax.plot(iters, sse, marker='o')\n",
    "    ax.set_xlabel('Cluster Centers')\n",
    "    ax.set_xticks(iters)\n",
    "    ax.set_xticklabels(iters)\n",
    "    ax.set_ylabel('SSE')\n",
    "    ax.set_title('SSE by Cluster Center Plot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_optimal_clusters(text, 30)\n",
    "\n",
    "clusters = MiniBatchKMeans(n_clusters=20, init_size=1024, batch_size=2048, random_state=20).fit_predict(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_tsne_pca(data, labels):\n",
    "    max_label = max(labels)\n",
    "    max_items = np.random.choice(range(data.shape[0]), size=3000, replace=False)\n",
    "    \n",
    "    pca = PCA(n_components=2).fit_transform(data[max_items,:].todense())\n",
    "    tsne = TSNE().fit_transform(PCA(n_components=50).fit_transform(data[max_items,:].todense()))\n",
    "    \n",
    "    \n",
    "    idx = np.random.choice(range(pca.shape[0]), size=1000, replace=False)\n",
    "    label_subset = labels[max_items]\n",
    "    label_subset = [cm.hsv(i/max_label) for i in label_subset[idx]]\n",
    "    \n",
    "    f, ax = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    \n",
    "    ax[0].scatter(pca[idx, 0], pca[idx, 1], c=label_subset)\n",
    "    ax[0].set_title('PCA Cluster Plot')\n",
    "    \n",
    "    ax[1].scatter(tsne[idx, 0], tsne[idx, 1], c=label_subset)\n",
    "    ax[1].set_title('TSNE Cluster Plot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tsne_pca(text, clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_keywords(data, clusters, labels, n_terms):\n",
    "    df = pd.DataFrame(data.todense()).groupby(clusters).mean()\n",
    "    \n",
    "    for i,r in df.iterrows():\n",
    "        print('\\nCluster {}'.format(i))\n",
    "        print(','.join([labels[t] for t in np.argsort(r)[-n_terms:]]))\n",
    "            \n",
    "\n",
    "### Look at Pros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_top_keywords(text, clusters, tfidf.get_feature_names(), 12)\n",
    "\n",
    "np.unique(clusters, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Look at Cons\n",
    "\n",
    "get_top_keywords(text, clusters, tfidf.get_feature_names(), 10)\n",
    "np.unique(clusters, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Calculate highest/lowest % companies per cluster\n",
    "\n",
    "# clusters array gives us cluster category per order of review in our dataframe\n",
    "print(len(clusters))\n",
    "clusters[:20] # preview\n",
    "data['cluster'] = clusters # make a new column in our OG dataframe\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that displays normalized top/bottom companies per cluster category\n",
    "def show_company_counts():\n",
    "    \n",
    "    # add cluster column to DF\n",
    "    data['cluster'] = clusters\n",
    "    \n",
    "    # multiindex series with review counts per company per cluster\n",
    "    grouped = data.groupby(['cluster','company']).title.count()\n",
    "    \n",
    "    # series of total reviews per company\n",
    "    total_counts = data.groupby('company').title.count()\n",
    "    \n",
    "    # multiindex DF resulting from joining above series together\n",
    "    final_grouped = pd.merge(grouped.reset_index(), total_counts.reset_index(), on=['company'], how='inner').set_index(['cluster','company'])\n",
    "    \n",
    "    # adding a column for normalized reviews: reviews per cluster/total number of reviews\n",
    "    final_grouped['normalized'] = final_grouped['title_x']/final_grouped['title_y']\n",
    "    \n",
    "    # return DF sorted by cluster and normalized value\n",
    "    return final_grouped.sort_values(by=['cluster','normalized'], ascending = [True,False])\n",
    "\n",
    "show_company_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_top_companies(cluster_array):\n",
    "    df = pd.DataFrame(columns = np.unique(cluster_array))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NMF Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "vectorizer = CountVectorizer(analyzer = \"word\", max_features = 10_000)\n",
    "\n",
    "pros_remove = all_reviews.pros.tolist()\n",
    "pros_remove_tokenized = [tokenize_sentences(text) for text in pros_remove]\n",
    "pros_remove_stopwords = [remove_stopwords(token) for token in pros_remove_tokenized]\n",
    "\n",
    "pros_joined = [' '.join(text) for text in pros_remove_stopwords]\n",
    "\n",
    "pros_joined = pros_joined\n",
    "word_counts = vectorizer.fit_transform(pros_joined)\n",
    "\n",
    "tfidf_transform = TfidfTransformer(smooth_idf = False)\n",
    "\n",
    "words_tfidf = tfidf_transform.fit_transform(word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_words = normalize(words_tfidf, norm = 'l1')\n",
    "\n",
    "model = NMF(n_components = 15, init = 'nndsvd')\n",
    "\n",
    "W = model.fit_transform(words_tfidf)\n",
    "\n",
    "W.shape\n",
    "\n",
    "H = model.components_\n",
    "\n",
    "H.shape\n",
    "\n",
    "lst = []\n",
    "\n",
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        message = \"Topic #%d: \" % topic_idx\n",
    "        message += \" \".join([feature_names[i]\n",
    "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        lst.append(message)\n",
    "    return lst\n",
    "\n",
    "topics_nmf = (print_top_words(model, vectorizer.get_feature_names(), 10))\n",
    "\n",
    "\n",
    "\n",
    "nmfsplit = [item.split() for item in topics_nmf]\n",
    "\n",
    "pd.DataFrame(nmfsplit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Look at Cons\n",
    "\n",
    "vectorizer = CountVectorizer(analyzer = \"word\", max_features = 10_000)\n",
    "\n",
    "cons_remove = all_reviews.cons.tolist()\n",
    "cons_remove_tokenized = [tokenize_sentences(text) for text in cons_remove]\n",
    "cons_remove_stopwords = [remove_stopwords(token) for token in cons_remove_tokenized]\n",
    "\n",
    "cons_joined = [' '.join(text) for text in cons_remove_stopwords]\n",
    "\n",
    "cons_joined = cons_joined\n",
    "word_counts = vectorizer.fit_transform(cons_joined)\n",
    "\n",
    "tfidf_transform = TfidfTransformer(smooth_idf = False)\n",
    "\n",
    "words_tfidf = tfidf_transform.fit_transform(word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NMF(n_components = 15, init = 'nndsvd')\n",
    "\n",
    "W = model.fit_transform(words_tfidf)\n",
    "\n",
    "W.shape\n",
    "\n",
    "H = model.components_\n",
    "\n",
    "H.shape\n",
    "\n",
    "lst = []\n",
    "\n",
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        message = \"Topic #%d: \" % topic_idx\n",
    "        message += \" \".join([feature_names[i]\n",
    "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        lst.append(message)\n",
    "    return lst\n",
    "\n",
    "topics_nmf = (print_top_words(model, vectorizer.get_feature_names(), 10))\n",
    "\n",
    "\n",
    "\n",
    "nmfsplit = [item.split() for item in topics_nmf]\n",
    "\n",
    "pd.DataFrame(nmfsplit)\n",
    "\n",
    "all_reviews.groupby('company').count()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
